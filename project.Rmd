---
title: "Project"
author: "Julian Gimbel"
date: "Sunday, July 26, 2015"
output: html_document
---

Start with an exploration
```{r}

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

summary(training)
training$classe

summary(testing)
cntr <- colnames(training)
cnte <- colnames(testing)

cntr[which(!cntr %in% cnte)]
cnte[which(!cnte %in% cntr)]

testing$problem_id
````
Remove describing columns
```{r}
training <- training[, -seq(from = 1, to = 8, by = 1)]
testing <- testing[, -seq(from = 1, to = 8, by = 1)]
```
Remove the columns that are NA.

```{r}
training <- training[,colSums(is.na(testing))<nrow(testing)]
testing <- testing[,colSums(is.na(testing))<nrow(testing)]
````

Using caret, rpart and random forest for the prediction and doParallel for lower waiting times.
```{r}
library(caret)

library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

tr <- train(classe ~ ., data = training, method = "rpart", trControl = fitControl)
tr
varImp(tr)



smpl <- sample(1:nrow(x = training), replace = FALSE, ceiling(nrow(x = training)/10))
````

Random Forest with all variables takes to long to compute. So computing the variable importance with a subset of the data an then using the most important variables for a full random forest.

```{r}
trrf <- train(classe ~ ., data = training[smpl,], method = "rf", trControl = fitControl)
trrf
vi <- varImp(trrf)

training <- training[,c("pitch_forearm","yaw_belt","magnet_dumbbell_z","pitch_belt","magnet_dumbbell_y","accel_belt_z","roll_forearm","magnet_belt_y","roll_dumbbell","accel_dumbbell_y","classe")]


trrf <- train(classe ~ ., data = training, method = "rf")
trrf
vi <- varImp(trrf)
vi
```
Random Forest does implicit cross validation, so there is no additional effort for that.
With an Prediction Accuracy of 98% the Random Forest modell fits pretty good to this data
The Out of sample error will therefor in the best case be arround 2% but more likely a bit more.

```{r}
pr <- predict(trrf, newdata = testing)

pr
```

Creating the files as demanded:

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pr)

```
